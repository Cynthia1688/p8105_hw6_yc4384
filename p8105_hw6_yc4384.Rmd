---
title: "p8105_hw6_yc4384"
author: "Yangyang Chen"
date: "`r Sys.Date()`"
output: github_document
---
```{r, include = FALSE, message = FALSE, warning = FALSE}
library(tidyverse)
library(viridis)
library(latex2exp)

knitr::opts_chunk$set(
	echo = TRUE,
	warning = FALSE,
	fig.width = 8, 
  fig.height = 6,
  out.width = "90%"
)

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d

theme_set(theme_minimal() + theme(legend.position = "bottom"))
```

# _Problem_1_
```{r library packages, echo=FALSE, results='hide', message=FALSE}
library(tidyverse)
```
* This dataset records the crimes solved rate in each state - The Washington Post has gathered data on homicides in 50 large U.S. cities.

## _Data Pre-processing_

* Add a character variable `city_state` representing `city` and `state` variables;

* Add a binary variable `resolved` to indicate whether cases were solved or not;

* Limit `victim_race` to `black` and `white` people;

* Filter cities `Tulsa, AL`, `Dallas, TX`, `Phoenix, AZ`, `Kansas City, MO` as they contain NA values;

* Drop NAs to guarantee `victim_age` is numeric.

```{r pre-processing, message=FALSE}
homicide_df = 
  read_csv("data/homicide-data.csv", na = c("", "NA", "Unknown")) |> 
  mutate(
    city_state = str_c(city, state, sep = ", "),
    victim_age = as.numeric(victim_age),
    resolved = as.numeric(disposition == "Closed by arrest")
  ) |> 
  filter(victim_race %in% c("Black", "White"),
         !(city_state %in% c("Tulsa, AL", "Dallas, TX", "Phoenix, AZ", "Kansas City, MO"))) |> 
  drop_na() |> 
  select(resolved, victim_age, victim_race, victim_sex, city_state)
```

## _Fitting Logistics Regression For Baltimore_

* We fitted a model using age, sex and race variables to compute odds ratio and confidence interval of solving homicides comparing male victims to female victims keeping all other variables fixed.

```{r Baltimore}
fit_logistic = 
  homicide_df |> 
  filter(city_state == "Baltimore, MD") |> 
  glm(resolved ~ victim_age + victim_sex + victim_race, data = _, family = binomial()) |>
  broom::tidy() |> 
  mutate(OR = exp(estimate),
         conf.low = exp(estimate - 1.96 * std.error),
         conf.high = exp(estimate + 1.96 * std.error)) |> 
  select(term, estimate, OR, starts_with("conf")) |> 
  knitr::kable(digits = 3)
fit_logistic
```

## _Fitting Logistics Regression For Each City_

* Then we used `map` function to obtain each city's Adjusted Odds Ratio and CIs for solving homicides comparing male victims to female victims.

```{r city}
city_logistic = 
  homicide_df |> 
  group_by(city_state)|> 
  nest(data = -city_state) |> 
  mutate(
    glm_logistics = map(.x = data, ~glm(resolved ~ victim_age + victim_sex + victim_race, data = .x, family = binomial())),
    tidy_tests = map(glm_logistics, broom::tidy)) |>  #mapping
  unnest(tidy_tests) |> 
  mutate(
    OR = exp(estimate),
    conf.low = exp(estimate - 1.96 * std.error),
    conf.high = exp(estimate + 1.96 * std.error)
  ) |> #CI
  select(city_state, term, OR, starts_with("conf.")) |> # table labels
  mutate(city_state = fct_reorder(city_state, OR)) |> 
  filter(term == "victim_sexMale") 
  
city_logistic |> 
  head() |> 
  knitr::kable(digits = 3)

```

## _Plotting for ORs and CIs in Each City_

```{r kogistic}
city_logistic |> 
  mutate(city_state = fct_reorder(city_state, OR)) |> 
  ggplot(aes(x = OR, y = city_state)) +
  geom_point() +
  geom_errorbar(aes(xmax = conf.high, xmin = conf.low)) +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))
```

* From the above plot, we observed that Washington, DC has the lowest odds ratio, indicating the lowest resolved homicide cases; while Albuquerque, NM, has the highest odds ratio. 

* Noticing that Fresno, CA has the widest confidence interval of OR among all cities, while Balttimore, MD has the narrowest confidence interval. Overall, there exists plenty of overlapping areas of CIs among all cities.

# _Problem 2_

## _Importing weather data_

* Codes are provided by this course:
```{r weather_df, cache = TRUE}
weather_df = 
  rnoaa::meteo_pull_monitors(
    c("USW00094728"),
    var = c("PRCP", "TMIN", "TMAX"), 
    date_min = "2022-01-01",
    date_max = "2022-12-31") |>
  mutate(
    name = recode(id, USW00094728 = "CentralPark_NY"),
    tmin = tmin / 10,
    tmax = tmax / 10) |>
  select(name, id, everything())
```

## _Plotting bootstrap sampling estimators using simple Linear regression model_

* To obtain a distribution for $\hat{r}^2$, we'll follow basically the same procedure we used for regression coefficients: draw bootstrap samples; the a model to each; extract the value I'm concerned with; and summarize. Here, we'll use `modelr::bootstrap` to draw the samples and `broom::glance` to produce `r.squared` values. 

```{r beta, cache = TRUE}
weather_df |> 
  modelr::bootstrap(n = 5000) |> 
  mutate(
    models = map(strap, \(df) lm(tmax ~ tmin + prcp, data = df)),
    results = map(models, broom::glance) 
  ) |> 
  select(results) |> 
  unnest(results) |> 
  ggplot(aes(x = r.squared)) +
  geom_density() +
  labs(title = TeX("Density Plot of $\\hat{r}^2$"), x = TeX("$\\hat{r}^2$"), y = TeX("Density" )) 
```

* The above plot shows that $\hat{r}^2$ is high, leading to the generally skewed shape of the density distribution. 

* If we wanted to construct a confidence interval for $R^2$, we could take the 2.5% and 97.5% quantiles of the estimates across bootstrap samples. Nevertheless, since the shape isn't symmetric, using the mean +/- 1.96 times the standard error probably wouldn't work well.

* We can produce a distribution for $\log(\beta_0 * \beta_1)$ using a similar approach, with a bit more wrangling before we make our plot.

```{r log_beta}
weather_df |> 
  modelr::bootstrap(n = 5000) |> 
  mutate(
    models = map(strap, \(df) lm(tmax ~ tmin + prcp, data = df)),
    results = map(models, broom::tidy)
  ) |> 
  select(-strap, -models) |> 
  unnest(results) |> 
  select(id = ".id", term, estimate) |> 
  pivot_wider(
    names_from = term,
    values_from = estimate
  ) |> 
  rename(beta0 = "(Intercept)", beta1 = tmin) |> 
  mutate(log_beta0_beta1 = log(beta0 * beta1)) |> 
  ggplot(aes(x = log_beta0_beta1)) +
  geom_density() +  
  labs(title = TeX("Density Plot of $\\log(\\beta_0*\\beta_1)$"), x = TeX("$\\log(\\beta_0*\\beta_1)$"), y = TeX("Density" )) 
  
```

* As with $r^2$, this distribution is somewhat skewed and has some outliers. 

# _Problem 3_